{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8533ddb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shirl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f5f870",
   "metadata": {},
   "source": [
    "# Kleine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee6de143",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-5f972c23edb1>:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df[\"Gecleand\"] = df['input'].str.replace('[^\\w\\s]','')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(\"C:\\\\Users\\\\shirl\\\\Desktop\\\\MovieReactionDS_small.json\")\n",
    "\n",
    "#CONTROLEREN OP NULL WAARDES\n",
    "df.isna().sum()\n",
    "\n",
    "#LEESTEKENS \n",
    "df[\"Gecleand\"] = df['input'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "#De kolom `Gecleand` alles in kleine letters zetten\n",
    "df['Gecleand'] = df['Gecleand'].str.lower()\n",
    "#print(df)\n",
    "\n",
    "#stopwoorden eruit halen\n",
    "stop = stopwords.words('english')\n",
    "df['Gecleand'] = df['Gecleand'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "#Positive eruit halen\n",
    "#data = df[df['output']== 'positive']\n",
    "data = df[['Gecleand', 'output']]\n",
    "\n",
    "data =str(data)\n",
    "\n",
    "#Tokenize\n",
    "corpus = []\n",
    "for i in sent_tokenize(data):\n",
    "    temp = []\n",
    "    for j in word_tokenize(i):\n",
    "        temp.append(j)\n",
    "    corpus.append(temp)\n",
    "    \n",
    "# Creating Word2Vec\n",
    "model = Word2Vec(\n",
    "    sentences = corpus,\n",
    "    window = 10)\n",
    "\n",
    "   \n",
    "#skip gram build vocabulary and train model\n",
    "#model_sg = gensim.models.Word2Vec(corpus, min_count = 1, vector_size = 100,\n",
    "                                             window = 10, sg = 1)\n",
    "#test = model_sg.wv['positive']\n",
    "\n",
    "#CBOW build vocabulary and train model\n",
    "#model_cbow = gensim.models.Word2Vec(corpus, min_count = 1,\n",
    "                              vector_size = 100, window = 5)\n",
    "#test2 = model_cbow.wv['positive']\n",
    "#print(test2)\n",
    "\n",
    "#model.predict_output_word(['movie'])\n",
    "model.predict_output_word('this is the best movie i ever saw', topn=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8e0e94",
   "metadata": {},
   "source": [
    "# Medium dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb6cd27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_json(\"C:\\\\Users\\\\shirl\\\\Desktop\\\\MovieReactionDS_medium.json\")\n",
    "\n",
    "#CONTROLEREN OP NULL WAARDES\n",
    "df.isna().sum()\n",
    "\n",
    "#LEESTEKENS \n",
    "df[\"Gecleand\"] = df['input'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "#De kolom `Gecleand` alles in kleine letters zetten\n",
    "df['Gecleand'] = df['Gecleand'].str.lower()\n",
    "\n",
    "#stopwoorden eruit halen\n",
    "stop = stopwords.words('english')\n",
    "df['Gecleand'] = df['Gecleand'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "#Positive eruit halen\n",
    "\n",
    "#data = df[df['output']== 'positive']\n",
    "data = df[['Gecleand', 'output']]\n",
    "print(data)\n",
    "data =str(data)\n",
    "\n",
    "print(data)\n",
    "\n",
    "#Tokenize\n",
    "corpus = []\n",
    "for i in sent_tokenize(data):\n",
    "    temp = []\n",
    "    for j in word_tokenize(i):\n",
    "        temp.append(j)\n",
    "    corpus.append(temp)\n",
    "    \n",
    "# Creating Word2Vec\n",
    "model = Word2Vec(\n",
    "    sentences = corpus, window = 10)\n",
    "\n",
    "   \n",
    "#print(\"Vector for positive:\")\n",
    "#print(model.wv[\"positive\"])\n",
    "#print()    \n",
    "    \n",
    "\n",
    "#skip gram build vocabulary and train model\n",
    "#model_sg = gensim.models.Word2Vec(corpus, min_count = 1, vector_size = 100,\n",
    "    #                                         window = 5, sg = 1)\n",
    "#test = model_sg.wv['positive']\n",
    "\n",
    "#CBOW build vocabulary and train model\n",
    "#model_cbow = gensim.models.Word2Vec(corpus, min_count = 1,\n",
    "  #                            vector_size = 100, window = 5)\n",
    "#test2 = model_cbow.wv['positive']\n",
    "#print(test2)\n",
    "\n",
    "test = model.predict_output_word([\"positive\"])\n",
    "print(test)\n",
    "\n",
    "test1 = model.predict(['positive'])\n",
    "print(test1)\n",
    "\n",
    "model_sg.predict_output_word(['positive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a951de",
   "metadata": {},
   "source": [
    "# Groot dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdd296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"C:\\\\Users\\\\shirl\\\\Desktop\\\\MovieReactionDS.json\")\n",
    "\n",
    "#CONTROLEREN OP NULL WAARDES\n",
    "df.isna().sum()\n",
    "\n",
    "#LEESTEKENS \n",
    "df[\"Gecleand\"] = df['input'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "#De kolom `Gecleand` alles in kleine letters zetten\n",
    "df['Gecleand'] = df['Gecleand'].str.lower()\n",
    "#print(df)\n",
    "\n",
    "#stopwoorden eruit halen\n",
    "stop = stopwords.words('english')\n",
    "df['Gecleand'] = df['Gecleand'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "print(df)\n",
    "\n",
    "#Positive eruit halen\n",
    "data = df[df['output']== 'positive']\n",
    "data =str(data)\n",
    "\n",
    "#Tokenize\n",
    "corpus = []\n",
    "for i in sent_tokenize(data):\n",
    "    temp = []\n",
    "    for j in word_tokenize(i):\n",
    "        temp.append(j)\n",
    "    corpus.append(temp)\n",
    "    \n",
    "# Creating Word2Vec\n",
    "model = Word2Vec(\n",
    "    sentences = corpus,\n",
    "    window = 10)\n",
    "\n",
    "   \n",
    "#print(\"Vector for positive:\")\n",
    "#print(model.wv[\"positive\"])\n",
    "#print()    \n",
    "    \n",
    "\n",
    "#skip gram build vocabulary and train model\n",
    "#model_sg = gensim.models.Word2Vec(corpus, min_count = 1, vector_size = 100,\n",
    " #                                            window = 5, sg = 1)\n",
    "#test = model_sg.wv['positive']\n",
    "\n",
    "#CBOW build vocabulary and train model\n",
    "#model_cbow = gensim.models.Word2Vec(corpus, min_count = 1,vector_size = 100, window = 5)\n",
    "#test2 = model_cbow.wv['positive']\n",
    "#print(test2)\n",
    "\n",
    "test = model.predict_output_word(['positive'])\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb68299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3844d3f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5bf228",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"C:\\\\Users\\\\shirl\\\\Desktop\\\\MovieReactionDS_small.json\")\n",
    "\n",
    "#CONTROLEREN OP NULL WAARDES\n",
    "df.isna().sum()\n",
    "\n",
    "#LEESTEKENS \n",
    "df[\"Gecleand\"] = df['input'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "#De kolom `Gecleand` alles in kleine letters zetten\n",
    "df['Gecleand'] = df['Gecleand'].str.lower()\n",
    "#print(df)\n",
    "\n",
    "#stopwoorden eruit halen\n",
    "stop = stopwords.words('english')\n",
    "df['Gecleand'] = df['Gecleand'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "print(df)\n",
    "\n",
    "#Positive eruit halen\n",
    "data = df[df['output']== 'positive']\n",
    "#data =str(data)\n",
    "\n",
    "#https://www.guru99.com/word-embedding-word2vec.html\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer=CountVectorizer()\n",
    "vocabulary=vectorizer.fit(data)\n",
    "X= vectorizer.transform(data)\n",
    "print(X.toarray())\n",
    "print(vocabulary.get_feature_names())\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "    sentences = corpus,\n",
    "    window = 10)\n",
    "\n",
    "\n",
    "w2v_model.build_vocab(data, progress_per=10000)\n",
    "\n",
    "#https://jaketae.github.io/study/word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0310a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.geeksforgeeks.org/implement-your-own-word2vecskip-gram-model-in-python/\n",
    "\n",
    "df = pd.read_json(\"C:\\\\Users\\\\shirl\\\\Desktop\\\\MovieReactionDS_small.json\")\n",
    "df.isna().sum()\n",
    "df[\"Gecleand\"] = df['input'].str.replace('[^\\w\\s]','')\n",
    "df['Gecleand'] = df['Gecleand'].str.lower()\n",
    "stop = stopwords.words('english')\n",
    "df['Gecleand'] = df['Gecleand'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "data = df[df['output']== 'positive']\n",
    "#data =str(data)\n",
    "\n",
    "#https://www.guru99.com/word-embedding-word2vec.html\n",
    "\n",
    "#w2v= Word2Vec(\n",
    "#    sentences = corpus,\n",
    "#    window = 10)\n",
    "\n",
    "test = {}\n",
    "for i in data:\n",
    "    for word in i:\n",
    "        if word not in test:\n",
    "            test[word] = 1\n",
    "        else:\n",
    "            test[word] += 1\n",
    "            \n",
    "v = len(test)\n",
    "test2 = sorted(list(test.keys()))\n",
    "#vocab = {}\n",
    "\n",
    "\n",
    "#for i in range(len(test)):\n",
    "#    print(i)\n",
    "#    vocab[test[i]] = i\n",
    "\n",
    "w2v = Word2Vec(\n",
    "    sentences = data,\n",
    "    window = 10)\n",
    "    \n",
    "for sentence in data:\n",
    "    for i in range(len(sentence)):\n",
    "        center_word = [0 for x in range(v)]\n",
    "        #center_word[vocab[sentence[i]]] ==1\n",
    "        context = [0 for x in range(v)]\n",
    "        \n",
    "        w2v.X_train.append(center_word)\n",
    "        w2v.y_train.append(context)\n",
    "        \n",
    "\n",
    "\n",
    "w2v.initialize(v,test2)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18083c44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91804bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
