{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8533ddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f5f870",
   "metadata": {},
   "source": [
    "# Kleine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ee6de143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                input    output  \\\n",
      "0   Supercraptastic slasher fare, which feels over...  negative   \n",
      "1   Even by the lowered standards of '80s slasher ...  negative   \n",
      "2   Foolish hikers go camping in the Utah mountain...  negative   \n",
      "3   Rented 3 bad movies to watch with my friends i...  negative   \n",
      "4   I was pulled into this movie early on, much to...  negative   \n",
      "5   The prey has an interesting history, unless yo...  negative   \n",
      "6   All the criticisms of this movie are quite val...  negative   \n",
      "7   I walked into a book store in Brentwood, Tenne...  negative   \n",
      "8   Thank God I didn't buy this movie myself! I bo...  negative   \n",
      "9   I saw this movie with my girlfriend. It was a ...  negative   \n",
      "10  If anyone tells you this picture is just terri...  negative   \n",
      "11  Are you kidding me? This is quite possibly the...  negative   \n",
      "12  In addition to the fact that this is just an a...  negative   \n",
      "13  I wanted to like this movie. I really, really ...  negative   \n",
      "14  I am not understanding why people are praising...  negative   \n",
      "15  This tale of the upper-classes getting their c...  negative   \n",
      "16  The filmmaker stayed true to the most accurate...  negative   \n",
      "17  I was able to see a preview of this movie thro...  positive   \n",
      "18  I saw The D's new film tonight at a special ad...  positive   \n",
      "19  I saw this movie on a fluke.I was standing on ...  positive   \n",
      "20  A wonder. My favorite film. The most important...  positive   \n",
      "21  Just saw it at as closing film of Austin Film ...  positive   \n",
      "22  I love a film that mixes edge-of-the-seat susp...  positive   \n",
      "23  There were heist movies before this one, and i...  positive   \n",
      "24  After 30+ years of hiatus, once again I immers...  positive   \n",
      "25  Meville's caper film is not as good as his mos...  positive   \n",
      "26  All men are guilty, says the chief of the poli...  positive   \n",
      "27  Jean-Pierre Melville's Le Cercle Rouge follows...  positive   \n",
      "\n",
      "                                             Gecleand  \n",
      "0   supercraptastic slasher fare feels overly long...  \n",
      "1   even lowered standards 80s slasher movies one ...  \n",
      "2   foolish hikers go camping utah mountains run m...  \n",
      "3   rented 3 bad movies watch friends dorm roomlep...  \n",
      "4   pulled movie early much surprise hadnt intende...  \n",
      "5   prey interesting history unless remember ads n...  \n",
      "6   criticisms movie quite valid pretty boring fil...  \n",
      "7   walked book store brentwood tennessee going sa...  \n",
      "8   thank god didnt buy movie borrowed friend boug...  \n",
      "9   saw movie girlfriend total disaster really see...  \n",
      "10  anyone tells picture terrific probably somethi...  \n",
      "11  kidding quite possibly worst amateur movie ive...  \n",
      "12  addition fact abysmally made film imagine givi...  \n",
      "13  wanted like movie really really excited saw pr...  \n",
      "14  understanding people praising movie didnt like...  \n",
      "15  tale upperclasses getting comeuppance wallowin...  \n",
      "16  filmmaker stayed true accurate account story p...  \n",
      "17  able see preview movie uclas prescreening prog...  \n",
      "18  saw ds new film tonight special advance screen...  \n",
      "19  saw movie flukei standing 42nd street waiting ...  \n",
      "20  wonder favorite film important film relationsh...  \n",
      "21  saw closing film austin film festival going se...  \n",
      "22  love film mixes edgeoftheseat suspense laughs ...  \n",
      "23  heist movies one indeed likes rififi obvious i...  \n",
      "24  30 years hiatus immerse mist uplifting melanch...  \n",
      "25  mevilles caper film good famous movie deliciou...  \n",
      "26  men guilty says chief police theyre born innoc...  \n",
      "27  jeanpierre melvilles le cercle rouge follows l...  \n",
      "Vector for positive:\n",
      "[-3.1397753e-06  3.2113108e-03 -6.8862825e-03 -1.3559908e-03\n",
      "  7.8034778e-03  7.3811766e-03 -3.6739311e-03  2.8009480e-03\n",
      " -8.4368540e-03  6.2113903e-03 -4.6087182e-03 -3.2072042e-03\n",
      "  9.4428789e-03  8.9015363e-04  7.6571158e-03 -6.1776070e-03\n",
      "  5.3407052e-03  1.0066615e-02 -8.7004807e-03 -5.2132783e-03\n",
      " -7.1224445e-03 -4.9514254e-03 -3.6996922e-03 -8.6144153e-03\n",
      "  8.0513619e-03 -4.9080662e-03  8.5854866e-03  5.4172929e-03\n",
      " -6.6832770e-03  4.0266425e-03  5.5614104e-03 -7.5906548e-03\n",
      " -7.5342944e-03 -2.5900409e-03 -8.7243160e-03 -1.5407711e-03\n",
      " -2.7954052e-04  3.2986752e-03  1.4208433e-03 -8.1373897e-04\n",
      " -5.6735794e-03  1.7942084e-03 -9.3963521e-04  6.8621309e-03\n",
      "  4.0873382e-03  4.6395124e-03  1.4628475e-03 -2.7180784e-03\n",
      " -4.4284915e-03 -1.0227294e-03  1.4809788e-03 -2.6931360e-03\n",
      " -7.2710826e-03 -7.9896785e-03 -9.2782909e-03 -6.0295523e-03\n",
      " -1.8167365e-03 -4.3385802e-03 -6.5397234e-03 -3.7473864e-03\n",
      "  4.3305708e-03 -3.7522037e-03  8.5395966e-03  1.4962249e-03\n",
      " -7.3201000e-03  9.5642982e-03  7.7706091e-03  5.6579006e-03\n",
      " -6.9698831e-03  5.8734827e-03  4.1283313e-03  5.2546766e-03\n",
      "  4.3222778e-03  2.0272995e-03 -3.2172692e-03  8.4368596e-03\n",
      "  9.7318143e-03  3.9016188e-03 -2.9177924e-03 -3.4287677e-05\n",
      "  1.1611447e-03 -8.5251061e-03 -8.2756514e-03 -2.0425596e-04\n",
      "  1.2169407e-03 -5.8595780e-03 -4.6961526e-03 -7.4697756e-03\n",
      "  8.4291827e-03  6.4908520e-05 -4.6227998e-03  5.7778358e-03\n",
      "  9.3425876e-03 -4.2102300e-03  8.2061701e-03  5.4838741e-03\n",
      "  5.9946617e-03  4.2353614e-04  8.2992967e-03 -7.1279467e-03]\n",
      "\n",
      "[ 1.6610617e-04  3.1847567e-03 -6.9366791e-03 -1.4911009e-03\n",
      "  7.7604032e-03  7.1135028e-03 -3.5233053e-03  2.9325113e-03\n",
      " -8.4013939e-03  6.0428991e-03 -4.5755929e-03 -3.3353232e-03\n",
      "  9.2742266e-03  9.6874521e-04  7.5784796e-03 -6.0830764e-03\n",
      "  5.2670483e-03  9.9379299e-03 -8.5471794e-03 -5.3209108e-03\n",
      " -7.0530898e-03 -4.8760222e-03 -3.6431311e-03 -8.7049613e-03\n",
      "  7.9589169e-03 -4.9331384e-03  8.4035341e-03  5.1271347e-03\n",
      " -6.6282325e-03  3.8782256e-03  5.6130290e-03 -7.4521950e-03\n",
      " -7.2437250e-03 -2.6153247e-03 -8.6304713e-03 -1.3856638e-03\n",
      " -3.6897120e-04  3.2399762e-03  1.3738861e-03 -1.0223081e-03\n",
      " -5.5509368e-03  1.6955249e-03 -9.5716940e-04  6.7563909e-03\n",
      "  4.0825978e-03  4.5892848e-03  1.3972026e-03 -2.7192445e-03\n",
      " -4.3767723e-03 -9.4726041e-04  1.5100712e-03 -2.8020025e-03\n",
      " -7.0858574e-03 -7.7796211e-03 -9.2052491e-03 -5.9401966e-03\n",
      " -1.7935442e-03 -4.3481719e-03 -6.5213530e-03 -3.6587138e-03\n",
      "  4.4304663e-03 -3.7583604e-03  8.4978351e-03  1.5822621e-03\n",
      " -7.3439782e-03  9.5864721e-03  7.6868860e-03  5.5703372e-03\n",
      " -7.0746667e-03  5.9932712e-03  3.9582355e-03  5.2089626e-03\n",
      "  4.3670945e-03  2.0309475e-03 -3.0315798e-03  8.3351368e-03\n",
      "  9.6263336e-03  3.7780672e-03 -2.9562393e-03  7.6194453e-08\n",
      "  1.1517054e-03 -8.3233630e-03 -8.3991373e-03 -1.4985455e-04\n",
      "  1.1273137e-03 -5.8304598e-03 -4.5327046e-03 -7.2954521e-03\n",
      "  8.4075304e-03  1.6681339e-04 -4.3739779e-03  5.7794759e-03\n",
      "  9.1420924e-03 -4.0991413e-03  8.1672762e-03  5.4354477e-03\n",
      "  5.9815021e-03  4.5970239e-04  8.1876675e-03 -7.1646273e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-64-f687dd64089f>:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df[\"Gecleand\"] = df['input'].str.replace('[^\\w\\s]','')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(\"C:\\\\Users\\\\shirl\\\\Desktop\\\\MovieReactionDS_small.json\")\n",
    "\n",
    "#CONTROLEREN OP NULL WAARDES\n",
    "df.isna().sum()\n",
    "\n",
    "#LEESTEKENS \n",
    "df[\"Gecleand\"] = df['input'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "#De kolom `Gecleand` alles in kleine letters zetten\n",
    "df['Gecleand'] = df['Gecleand'].str.lower()\n",
    "#print(df)\n",
    "\n",
    "#stopwoorden eruit halen\n",
    "stop = stopwords.words('english')\n",
    "df['Gecleand'] = df['Gecleand'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "print(df)\n",
    "\n",
    "#Positive eruit halen\n",
    "data = df[df['output']== 'positive']\n",
    "data =str(data)\n",
    "\n",
    "#Tokenize\n",
    "corpus = []\n",
    "for i in sent_tokenize(data):\n",
    "    temp = []\n",
    "    for j in word_tokenize(i):\n",
    "        temp.append(j)\n",
    "    corpus.append(temp)\n",
    "    \n",
    "# Creating Word2Vec\n",
    "model = Word2Vec(\n",
    "    sentences = corpus,\n",
    "    window = 10)\n",
    "\n",
    "   \n",
    "print(\"Vector for positive:\")\n",
    "print(model.wv[\"positive\"])\n",
    "print()    \n",
    "    \n",
    "\n",
    "#skip gram build vocabulary and train model\n",
    "model_sg = gensim.models.Word2Vec(corpus, min_count = 1, vector_size = 100,\n",
    "                                             window = 5, sg = 1)\n",
    "test = model_sg.wv['positive']\n",
    "\n",
    "#CBOW build vocabulary and train model\n",
    "model_cbow = gensim.models.Word2Vec(corpus, min_count = 1,\n",
    "                              vector_size = 100, window = 5)\n",
    "test2 = model_cbow.wv['positive']\n",
    "print(test2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8e0e94",
   "metadata": {},
   "source": [
    "# Medium dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ccb6cd27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-65-2cef9c296b54>:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df[\"Gecleand\"] = df['input'].str.replace('[^\\w\\s]','')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 input    output  \\\n",
      "0    Once again Mr. Costner has dragged out a movie...  negative   \n",
      "1    This is an example of why the majority of acti...  negative   \n",
      "2    First of all I hate those moronic rappers, who...  negative   \n",
      "3    Not even the Beatles could write songs everyon...  negative   \n",
      "4    Brass pictures (movies is not a fitting word f...  negative   \n",
      "..                                                 ...       ...   \n",
      "295  I was extraordinarily impressed by this film. ...  positive   \n",
      "296  Although I'm not a golf fan, I attended a snea...  positive   \n",
      "297  From the start of \"The Edge Of Love\", the view...  positive   \n",
      "298  This movie, with all its complexity and subtle...  positive   \n",
      "299  I've seen this story before but my kids haven'...  positive   \n",
      "\n",
      "                                              Gecleand  \n",
      "0    mr costner dragged movie far longer necessary ...  \n",
      "1    example majority action films generic boring t...  \n",
      "2    first hate moronic rappers couldnt act gun pre...  \n",
      "3    even beatles could write songs everyone liked ...  \n",
      "4    brass pictures movies fitting word really some...  \n",
      "..                                                 ...  \n",
      "295  extraordinarily impressed film one best sports...  \n",
      "296  although im golf fan attended sneak preview mo...  \n",
      "297  start edge love viewer transported striking wo...  \n",
      "298  movie complexity subtlety makes one thoughtpro...  \n",
      "299  ive seen story kids havent boy troubled past j...  \n",
      "\n",
      "[300 rows x 3 columns]\n",
      "Vector for positive:\n",
      "[-0.00868866  0.00369485  0.00518462  0.00579582  0.00745862 -0.00618813\n",
      "  0.00115356  0.00613939 -0.00290653 -0.00616957 -0.0003962  -0.00837093\n",
      " -0.00567019  0.00716832  0.00331917  0.00719584  0.00683131  0.00759289\n",
      " -0.00383732 -0.00065154  0.0023965  -0.00455186  0.00849695 -0.00982591\n",
      "  0.00680625  0.00289909 -0.00492819  0.00448556 -0.00184598  0.00670957\n",
      "  0.00998414 -0.00437904 -0.00055006 -0.00581153  0.00388304  0.0027517\n",
      "  0.00694915  0.00609439  0.00954812  0.00927864  0.00785904 -0.00700301\n",
      " -0.00928065 -0.00032811 -0.00306023  0.00787992  0.00592571 -0.00149217\n",
      "  0.00156353  0.00181002  0.00781033 -0.00948129 -0.00019596  0.00343758\n",
      " -0.00091236  0.0083226   0.0090048   0.00645989 -0.00074502  0.00774212\n",
      " -0.00852988  0.00318157 -0.00464199 -0.00514396  0.00356318  0.00539956\n",
      "  0.00784521 -0.00567789  0.00739562  0.00670319 -0.00370318 -0.00867487\n",
      "  0.00542732  0.00649572 -0.00075099 -0.00667562 -0.00705861 -0.00244746\n",
      "  0.00517016 -0.00377991 -0.00944318  0.00380513  0.00493865 -0.00641053\n",
      "  0.00113662 -0.00212269  0.00011139 -0.00992947  0.00263557 -0.00475198\n",
      "  0.0011047  -0.00159773  0.00226447 -0.00792843 -0.002627    0.00266367\n",
      "  0.00534884 -0.00242137 -0.00951162  0.00455843]\n",
      "\n",
      "[-0.00867123  0.00374802  0.00516675  0.00567963  0.0074959  -0.00620269\n",
      "  0.00119781  0.00619538 -0.00297189 -0.0062501  -0.00040233 -0.00857859\n",
      " -0.00560734  0.00716292  0.00336676  0.0071751   0.00685744  0.00743789\n",
      " -0.00382537 -0.00076021  0.00238274 -0.00446524  0.00840229 -0.00996603\n",
      "  0.00673636  0.00285495 -0.00502308  0.00425946 -0.0018607   0.00664697\n",
      "  0.01003529 -0.00435289 -0.00055051 -0.00582694  0.0037802   0.00288589\n",
      "  0.00687645  0.00605426  0.00953321  0.00908082  0.00798651 -0.00704869\n",
      " -0.00917427 -0.0003287  -0.00302399  0.00788472  0.00591165 -0.00159619\n",
      "  0.00157404  0.00188186  0.00777641 -0.00961166 -0.00012312  0.00351994\n",
      " -0.00097344  0.0084291   0.00901748  0.00655366 -0.00085013  0.00773339\n",
      " -0.00842337  0.00324832 -0.00458193 -0.00504542  0.00347293  0.00547399\n",
      "  0.00778444 -0.0057123   0.0073101   0.00673022 -0.0037422  -0.00861562\n",
      "  0.0055974   0.00648817 -0.00065889 -0.0067488  -0.00707201 -0.0024846\n",
      "  0.00505563 -0.00365688 -0.00940103  0.00385307  0.00481331 -0.0062906\n",
      "  0.00113123 -0.00213737  0.00015267 -0.00981262  0.00274525 -0.00468585\n",
      "  0.00120934 -0.00151651  0.00220995 -0.00784944 -0.00264728  0.00278031\n",
      "  0.00536345 -0.00238926 -0.0095228   0.00453549]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(\"C:\\\\Users\\\\shirl\\\\Desktop\\\\MovieReactionDS_medium.json\")\n",
    "\n",
    "#CONTROLEREN OP NULL WAARDES\n",
    "df.isna().sum()\n",
    "\n",
    "#LEESTEKENS \n",
    "df[\"Gecleand\"] = df['input'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "#De kolom `Gecleand` alles in kleine letters zetten\n",
    "df['Gecleand'] = df['Gecleand'].str.lower()\n",
    "#print(df)\n",
    "\n",
    "#stopwoorden eruit halen\n",
    "stop = stopwords.words('english')\n",
    "df['Gecleand'] = df['Gecleand'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "print(df)\n",
    "\n",
    "#Positive eruit halen\n",
    "data = df[df['output']== 'positive']\n",
    "data =str(data)\n",
    "\n",
    "#Tokenize\n",
    "corpus = []\n",
    "for i in sent_tokenize(data):\n",
    "    temp = []\n",
    "    for j in word_tokenize(i):\n",
    "        temp.append(j)\n",
    "    corpus.append(temp)\n",
    "    \n",
    "# Creating Word2Vec\n",
    "model = Word2Vec(\n",
    "    sentences = corpus,\n",
    "    window = 10)\n",
    "\n",
    "   \n",
    "print(\"Vector for positive:\")\n",
    "print(model.wv[\"positive\"])\n",
    "print()    \n",
    "    \n",
    "\n",
    "#skip gram build vocabulary and train model\n",
    "model_sg = gensim.models.Word2Vec(corpus, min_count = 1, vector_size = 100,\n",
    "                                             window = 5, sg = 1)\n",
    "test = model_sg.wv['positive']\n",
    "\n",
    "#CBOW build vocabulary and train model\n",
    "model_cbow = gensim.models.Word2Vec(corpus, min_count = 1,\n",
    "                              vector_size = 100, window = 5)\n",
    "test2 = model_cbow.wv['positive']\n",
    "print(test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a951de",
   "metadata": {},
   "source": [
    "# Groot dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2bdd296a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-66-67624d59b47c>:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df[\"Gecleand\"] = df['input'].str.replace('[^\\w\\s]','')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   input    output  \\\n",
      "0      Once again Mr. Costner has dragged out a movie...  negative   \n",
      "1      This is an example of why the majority of acti...  negative   \n",
      "2      First of all I hate those moronic rappers, who...  negative   \n",
      "3      Not even the Beatles could write songs everyon...  negative   \n",
      "4      Brass pictures (movies is not a fitting word f...  negative   \n",
      "...                                                  ...       ...   \n",
      "24995  I was extraordinarily impressed by this film. ...  positive   \n",
      "24996  Although I'm not a golf fan, I attended a snea...  positive   \n",
      "24997  From the start of \"The Edge Of Love\", the view...  positive   \n",
      "24998  This movie, with all its complexity and subtle...  positive   \n",
      "24999  I've seen this story before but my kids haven'...  positive   \n",
      "\n",
      "                                                Gecleand  \n",
      "0      mr costner dragged movie far longer necessary ...  \n",
      "1      example majority action films generic boring t...  \n",
      "2      first hate moronic rappers couldnt act gun pre...  \n",
      "3      even beatles could write songs everyone liked ...  \n",
      "4      brass pictures movies fitting word really some...  \n",
      "...                                                  ...  \n",
      "24995  extraordinarily impressed film one best sports...  \n",
      "24996  although im golf fan attended sneak preview mo...  \n",
      "24997  start edge love viewer transported striking wo...  \n",
      "24998  movie complexity subtlety makes one thoughtpro...  \n",
      "24999  ive seen story kids havent boy troubled past j...  \n",
      "\n",
      "[25000 rows x 3 columns]\n",
      "Vector for positive:\n",
      "[-8.6210677e-03  3.6663571e-03  5.1928968e-03  5.7469057e-03\n",
      "  7.4635972e-03 -6.1717583e-03  1.1087571e-03  6.0523106e-03\n",
      " -2.8428538e-03 -6.1760410e-03 -4.0685417e-04 -8.3707953e-03\n",
      " -5.6027924e-03  7.1085524e-03  3.3507654e-03  7.2257062e-03\n",
      "  6.8025347e-03  7.5322846e-03 -3.7936189e-03 -5.6636351e-04\n",
      "  2.3520507e-03 -4.5172889e-03  8.3929673e-03 -9.8591754e-03\n",
      "  6.7685805e-03  2.9131556e-03 -4.9338462e-03  4.4015180e-03\n",
      " -1.7433948e-03  6.7104506e-03  9.9626780e-03 -4.3635201e-03\n",
      " -5.9500732e-04 -5.6998436e-03  3.8501525e-03  2.7860678e-03\n",
      "  6.8957675e-03  6.0991407e-03  9.5397802e-03  9.2723919e-03\n",
      "  7.8945421e-03 -6.9880541e-03 -9.1611836e-03 -3.5780031e-04\n",
      " -3.1002280e-03  7.8952527e-03  5.9357672e-03 -1.5413760e-03\n",
      "  1.5134604e-03  1.7945992e-03  7.8147789e-03 -9.5093483e-03\n",
      " -2.0756484e-04  3.4699657e-03 -9.3518983e-04  8.3807204e-03\n",
      "  9.0140672e-03  6.5341354e-03 -7.1344030e-04  7.7157919e-03\n",
      " -8.5361563e-03  3.2076435e-03 -4.6404717e-03 -5.0932160e-03\n",
      "  3.5893142e-03  5.3722872e-03  7.7701830e-03 -5.7646134e-03\n",
      "  7.4329833e-03  6.6274768e-03 -3.7076881e-03 -8.7428195e-03\n",
      "  5.4375357e-03  6.5063024e-03 -7.8563171e-04 -6.7103910e-03\n",
      " -7.0832735e-03 -2.4977345e-03  5.1426631e-03 -3.6698268e-03\n",
      " -9.3716774e-03  3.8284909e-03  4.8875799e-03 -6.4260969e-03\n",
      "  1.2060502e-03 -2.0743296e-03  2.7220702e-05 -9.8871235e-03\n",
      "  2.6909732e-03 -4.7475365e-03  1.0885136e-03 -1.5762909e-03\n",
      "  2.1986747e-03 -7.8825438e-03 -2.7129527e-03  2.6659709e-03\n",
      "  5.3432491e-03 -2.3951291e-03 -9.5108422e-03  4.5094048e-03]\n",
      "\n",
      "[-0.00861228  0.0036803   0.00514396  0.00570495  0.00749922 -0.00631844\n",
      "  0.00119205  0.00621246 -0.00290293 -0.00628091 -0.00042624 -0.00851621\n",
      " -0.00561471  0.0072099   0.00341861  0.00720395  0.0068223   0.00746221\n",
      " -0.00379764 -0.00068463  0.00239628 -0.00445288  0.00843871 -0.0099449\n",
      "  0.00676829  0.00291078 -0.00495589  0.00436701 -0.00180159  0.00668782\n",
      "  0.0100549  -0.00434128 -0.00046674 -0.00575205  0.00379314  0.00293317\n",
      "  0.00691874  0.00606519  0.0094641   0.0091651   0.00790409 -0.00700947\n",
      " -0.00918101 -0.00039364 -0.00300418  0.00789547  0.00588956 -0.00156042\n",
      "  0.0015284   0.00185339  0.0078673  -0.00951417 -0.00016927  0.00345027\n",
      " -0.00094336  0.00839976  0.0090066   0.00655387 -0.00079337  0.00770235\n",
      " -0.00847852  0.00321185 -0.00461224 -0.00505162  0.00353483  0.00547179\n",
      "  0.00779439 -0.00573475  0.00728301  0.00677612 -0.00371153 -0.00869599\n",
      "  0.00555139  0.00652237 -0.00067646 -0.00671776 -0.0070251  -0.00247657\n",
      "  0.00508714 -0.00365108 -0.00943098  0.00385206  0.00477952 -0.00636089\n",
      "  0.00119615 -0.00211412  0.00016571 -0.00982148  0.002751   -0.00472617\n",
      "  0.00117668 -0.001486    0.00221755 -0.00787801 -0.00261058  0.00274232\n",
      "  0.00538605 -0.00243693 -0.00949625  0.00447313]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(\"C:\\\\Users\\\\shirl\\\\Desktop\\\\MovieReactionDS.json\")\n",
    "\n",
    "#CONTROLEREN OP NULL WAARDES\n",
    "df.isna().sum()\n",
    "\n",
    "#LEESTEKENS \n",
    "df[\"Gecleand\"] = df['input'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "#De kolom `Gecleand` alles in kleine letters zetten\n",
    "df['Gecleand'] = df['Gecleand'].str.lower()\n",
    "#print(df)\n",
    "\n",
    "#stopwoorden eruit halen\n",
    "stop = stopwords.words('english')\n",
    "df['Gecleand'] = df['Gecleand'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "print(df)\n",
    "\n",
    "#Positive eruit halen\n",
    "data = df[df['output']== 'positive']\n",
    "data =str(data)\n",
    "\n",
    "#Tokenize\n",
    "corpus = []\n",
    "for i in sent_tokenize(data):\n",
    "    temp = []\n",
    "    for j in word_tokenize(i):\n",
    "        temp.append(j)\n",
    "    corpus.append(temp)\n",
    "    \n",
    "# Creating Word2Vec\n",
    "model = Word2Vec(\n",
    "    sentences = corpus,\n",
    "    window = 10)\n",
    "\n",
    "   \n",
    "print(\"Vector for positive:\")\n",
    "print(model.wv[\"positive\"])\n",
    "print()    \n",
    "    \n",
    "\n",
    "#skip gram build vocabulary and train model\n",
    "model_sg = gensim.models.Word2Vec(corpus, min_count = 1, vector_size = 100,\n",
    "                                             window = 5, sg = 1)\n",
    "test = model_sg.wv['positive']\n",
    "\n",
    "#CBOW build vocabulary and train model\n",
    "model_cbow = gensim.models.Word2Vec(corpus, min_count = 1,\n",
    "                              vector_size = 100, window = 5)\n",
    "test2 = model_cbow.wv['positive']\n",
    "print(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb68299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3844d3f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5bf228",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
